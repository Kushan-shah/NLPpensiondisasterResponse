{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "081ea31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 1: TEXT NORMALIZATION ENGINE (ABSOLUTE PEAK)\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class TextNormalizer:\n",
    "    \"\"\"\n",
    "    High-assurance, deterministic text normalization engine\n",
    "    for noisy pension & disaster-response documents.\n",
    "\n",
    "    Properties:\n",
    "    - OCR-robust\n",
    "    - Unicode-safe\n",
    "    - Domain-aware\n",
    "    - Word-boundary safe\n",
    "    - Explainable (trace)\n",
    "    - Confidence-aware (noise score)\n",
    "    - Zero ML, zero ambiguity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # ----------------------------------------------------\n",
    "        # DOMAIN-SAFE CANONICAL REPLACEMENTS\n",
    "        # ----------------------------------------------------\n",
    "        self.replacements = {\n",
    "            r\"\\byrs\\b\": \"years\",\n",
    "            r\"\\byr\\b\": \"year\",\n",
    "            r\"\\bage\\b\": \"age\",\n",
    "            r\"\\bgovt\\b\": \"government\",\n",
    "            r\"\\bdept\\b\": \"department\",\n",
    "            r\"\\bdistt\\b\": \"district\",\n",
    "            r\"\\baddr\\b\": \"address\",\n",
    "            r\"\\bwid\\b\": \"widow\",\n",
    "            r\"\\bexpired\\b\": \"passed away\",\n",
    "            r\"\\bno income\\b\": \"no income\",\n",
    "            r\"\\bjobless\\b\": \"no income\"\n",
    "        }\n",
    "\n",
    "        self._compiled_replacements = [\n",
    "            (re.compile(p), r) for p, r in self.replacements.items()\n",
    "        ]\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # STRUCTURAL & NUMERIC NORMALIZATION\n",
    "        # ----------------------------------------------------\n",
    "        self.age_pattern = re.compile(\n",
    "            r\"\\b(\\d{1,3})\\s*[-]?\\s*(year|years|yrs)\\b\"\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # UNICODE & OCR NOISE CONTROL\n",
    "        # ----------------------------------------------------\n",
    "        self.illegal_chars = re.compile(\n",
    "            r\"[^\\w\\s\\.\\,\\;\\:\\!\\?\\-]\"\n",
    "        )\n",
    "\n",
    "        self.repeated_punct = re.compile(\n",
    "            r\"([.!?]){2,}\"\n",
    "        )\n",
    "\n",
    "        self.field_separator = re.compile(\n",
    "            r\"\\s*[:\\-]\\s*\"\n",
    "        )\n",
    "\n",
    "        self.whitespace = re.compile(r\"\\s+\")\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # HARD NOISE INDICATORS (CONFIDENCE PENALTY)\n",
    "        # ----------------------------------------------------\n",
    "        self.noise_patterns = [\n",
    "            re.compile(p) for p in [\n",
    "                r\"\\?\\?\\?\",\n",
    "                r\"unknown\",\n",
    "                r\"illegible\",\n",
    "                r\"n/a\",\n",
    "                r\"xxx\",\n",
    "                r\"cannot read\",\n",
    "                r\"blurred\"\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    # ========================================================\n",
    "    # NORMALIZATION PIPELINE\n",
    "    # ========================================================\n",
    "\n",
    "    def normalize(self, text: str) -> Dict[str, object]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        {\n",
    "            'normalized_text': str,\n",
    "            'normalization_trace': List[str],\n",
    "            'noise_score': float\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        trace: List[str] = []\n",
    "        noise_score = 0.0\n",
    "\n",
    "        # 0. Unicode canonicalization (OCR & PDF safe)\n",
    "        text = unicodedata.normalize(\"NFKD\", text)\n",
    "        trace.append(\"unicode normalized\")\n",
    "\n",
    "        # 1. Case normalization\n",
    "        text = text.lower()\n",
    "        trace.append(\"lowercased\")\n",
    "\n",
    "        # 2. Normalize field separators (semi-structured forms)\n",
    "        if self.field_separator.search(text):\n",
    "            text = self.field_separator.sub(\": \", text)\n",
    "            trace.append(\"normalized field separators\")\n",
    "\n",
    "        # 3. Remove illegal / OCR characters\n",
    "        if self.illegal_chars.search(text):\n",
    "            text = self.illegal_chars.sub(\" \", text)\n",
    "            trace.append(\"removed OCR noise\")\n",
    "\n",
    "        # 4. Collapse repeated punctuation\n",
    "        if self.repeated_punct.search(text):\n",
    "            text = self.repeated_punct.sub(r\"\\1\", text)\n",
    "            trace.append(\"collapsed repeated punctuation\")\n",
    "\n",
    "        # 5. Canonical lexical replacements (boundary-safe)\n",
    "        for pattern, replacement in self._compiled_replacements:\n",
    "            if pattern.search(text):\n",
    "                text = pattern.sub(replacement, text)\n",
    "                trace.append(f\"canonicalized {pattern.pattern}\")\n",
    "\n",
    "        # 6. Normalize numeric age expressions\n",
    "        if self.age_pattern.search(text):\n",
    "            text = self.age_pattern.sub(r\"\\1 years\", text)\n",
    "            trace.append(\"normalized age expressions\")\n",
    "\n",
    "        # 7. Whitespace normalization\n",
    "        text = self.whitespace.sub(\" \", text).strip()\n",
    "        trace.append(\"normalized whitespace\")\n",
    "\n",
    "        # 8. Noise scoring (confidence penalty)\n",
    "        for p in self.noise_patterns:\n",
    "            if p.search(text):\n",
    "                noise_score += 0.15\n",
    "\n",
    "        noise_score = min(noise_score, 1.0)\n",
    "\n",
    "        return {\n",
    "            \"normalized_text\": text,\n",
    "            \"normalization_trace\": trace,\n",
    "            \"noise_score\": round(noise_score, 2)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "06c4b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PART 2 — FINAL REFINED ADAPTIVE SEMANTIC LEXICON SYSTEM\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "# ============================================================\n",
    "# SEMANTIC ROLE ONTOLOGY\n",
    "# ============================================================\n",
    "\n",
    "SEMANTIC_ROLES = {\n",
    "    \"AGE\", \"LOCATION\", \"DISASTER\",\n",
    "    \"WIDOW\", \"DISABILITY\", \"OLD_AGE\", \"DISTRESS\"\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# SEED SEMANTIC LEXICON (IMMUTABLE ANCHORS)\n",
    "# ============================================================\n",
    "\n",
    "class SeedSemanticLexicon:\n",
    "    \"\"\"\n",
    "    High-precision semantic anchors.\n",
    "    Seeds are immutable and role-specific.\n",
    "    \"\"\"\n",
    "\n",
    "    AGE_PATTERNS = [\n",
    "        r\"\\b(\\d{2})\\s+year[s]?\\s+old\\b\",\n",
    "        r\"\\bat\\s+the\\s+age\\s+of\\s+(\\d{2})\\b\",\n",
    "        r\"\\baged\\s+(\\d{2})\\b\",\n",
    "        r\"\\bi\\s+am\\s+(\\d{2})\\s+years\\b\"\n",
    "    ]\n",
    "\n",
    "    # ✅ FIX: LOCATION PATTERN (REQUIRED BY EXTRACTOR)\n",
    "    LOCATION_PATTERN = r\"\\b(live in\\s+)?([a-z ]{3,})\\s+(district|city|village)\\b\"\n",
    "\n",
    "    DISASTER_SEEDS = {\n",
    "        \"Flood\": {\"flood\", \"floods\", \"flooding\"},\n",
    "        \"Cyclone\": {\"cyclone\", \"storm\", \"cyclonic\"},\n",
    "        \"Fire\": {\"fire\", \"burnt\", \"burned\"},\n",
    "        \"Earthquake\": {\"earthquake\", \"tremor\", \"seismic\"}\n",
    "    }\n",
    "\n",
    "    WIDOW_SEEDS = {\n",
    "        \"widow\", \"widowed\",\n",
    "        \"husband passed away\",\n",
    "        \"lost my husband\"\n",
    "    }\n",
    "\n",
    "    DISABILITY_SEEDS = {\n",
    "        \"disabled\",\n",
    "        \"unable to work\",\n",
    "        \"physically impaired\",\n",
    "        \"medically unfit\"\n",
    "    }\n",
    "\n",
    "    OLD_AGE_SEEDS = {\n",
    "        \"too old to work\",\n",
    "        \"elderly\",\n",
    "        \"advanced age\"\n",
    "    }\n",
    "\n",
    "    DISTRESS_SEEDS = {\n",
    "        \"no income\",\n",
    "        \"lost everything\",\n",
    "        \"urgent help\",\n",
    "        \"dependent on support\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LEXICON MEMORY (STABLE, SERIALIZABLE)\n",
    "# ============================================================\n",
    "\n",
    "class LexiconMemory:\n",
    "    \"\"\"\n",
    "    Stores learned phrases with stability metadata.\n",
    "    Safe for persistence inside model pickle.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.memory: Dict[str, Dict[str, dict]] = defaultdict(dict)\n",
    "\n",
    "    def add(self, role: str, phrase: str, meta: dict):\n",
    "        if phrase not in self.memory[role]:\n",
    "            self.memory[role][phrase] = meta\n",
    "        else:\n",
    "            self.memory[role][phrase][\"freq\"] += meta[\"freq\"]\n",
    "            self.memory[role][phrase][\"docs\"] += meta[\"docs\"]\n",
    "\n",
    "    def get(\n",
    "        self,\n",
    "        role: str,\n",
    "        min_freq: int = 3,\n",
    "        min_docs: int = 2,\n",
    "        min_purity: float = 0.7\n",
    "    ) -> Set[str]:\n",
    "        return {\n",
    "            p for p, m in self.memory[role].items()\n",
    "            if (\n",
    "                m[\"freq\"] >= min_freq and\n",
    "                m[\"docs\"] >= min_docs and\n",
    "                m[\"purity\"] >= min_purity\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def is_cross_role(self, phrase: str, role: str) -> bool:\n",
    "        for r, phrases in self.memory.items():\n",
    "            if r != role and phrase in phrases:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STABILITY-AWARE LEXICON EXPANSION ENGINE\n",
    "# ============================================================\n",
    "\n",
    "class StableLexiconExpansionEngine:\n",
    "    \"\"\"\n",
    "    Adaptive lexicon expansion using:\n",
    "    - frequency\n",
    "    - role purity\n",
    "    - causal context validation\n",
    "    - cross-role isolation\n",
    "    \"\"\"\n",
    "\n",
    "    CAUSAL_MARKERS = {\n",
    "        \"because\", \"due to\", \"after\", \"since\", \"as a result\"\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_freq: int = 3,\n",
    "        min_role_purity: float = 0.7,\n",
    "        max_ngram: int = 5\n",
    "    ):\n",
    "        self.min_freq = min_freq\n",
    "        self.min_role_purity = min_role_purity\n",
    "        self.max_ngram = max_ngram\n",
    "\n",
    "    def mine_candidates(\n",
    "        self,\n",
    "        clauses: List[str],\n",
    "        seeds: Set[str]\n",
    "    ) -> Dict[str, List[str]]:\n",
    "\n",
    "        candidates = defaultdict(list)\n",
    "\n",
    "        for clause in clauses:\n",
    "            if not any(seed in clause for seed in seeds):\n",
    "                continue\n",
    "\n",
    "            tokens = clause.split()\n",
    "            for n in range(2, self.max_ngram + 1):\n",
    "                for i in range(len(tokens) - n + 1):\n",
    "                    phrase = \" \".join(tokens[i:i+n])\n",
    "                    candidates[phrase].append(clause)\n",
    "\n",
    "        return candidates\n",
    "\n",
    "    def role_purity(self, contexts: List[str], seeds: Set[str]) -> float:\n",
    "        return sum(\n",
    "            1 for c in contexts if any(s in c for s in seeds)\n",
    "        ) / len(contexts)\n",
    "\n",
    "    def causal_hits(self, contexts: List[str]) -> int:\n",
    "        return sum(\n",
    "            1 for c in contexts\n",
    "            if any(m in c for m in self.CAUSAL_MARKERS)\n",
    "        )\n",
    "\n",
    "    def promote(\n",
    "        self,\n",
    "        candidates: Dict[str, List[str]],\n",
    "        role: str,\n",
    "        seeds: Set[str],\n",
    "        memory: LexiconMemory\n",
    "    ):\n",
    "        for phrase, contexts in candidates.items():\n",
    "\n",
    "            if len(contexts) < self.min_freq:\n",
    "                continue\n",
    "\n",
    "            purity = self.role_purity(contexts, seeds)\n",
    "            if purity < self.min_role_purity:\n",
    "                continue\n",
    "\n",
    "            if self.causal_hits(contexts) == 0:\n",
    "                continue\n",
    "\n",
    "            if memory.is_cross_role(phrase, role):\n",
    "                continue\n",
    "\n",
    "            memory.add(role, phrase, {\n",
    "                \"freq\": len(contexts),\n",
    "                \"docs\": len(set(contexts)),\n",
    "                \"purity\": round(purity, 2)\n",
    "            })\n",
    "\n",
    "    def update_from_text(self, text, segmenter, memory: LexiconMemory):\n",
    "        clauses = [\n",
    "            c for s in segmenter.split_sentences(text)\n",
    "            for c in segmenter.split_clauses(s)\n",
    "        ]\n",
    "\n",
    "        self.promote(\n",
    "            self.mine_candidates(clauses, SeedSemanticLexicon.WIDOW_SEEDS),\n",
    "            \"WIDOW\", SeedSemanticLexicon.WIDOW_SEEDS, memory\n",
    "        )\n",
    "        self.promote(\n",
    "            self.mine_candidates(clauses, SeedSemanticLexicon.DISABILITY_SEEDS),\n",
    "            \"DISABILITY\", SeedSemanticLexicon.DISABILITY_SEEDS, memory\n",
    "        )\n",
    "        self.promote(\n",
    "            self.mine_candidates(clauses, SeedSemanticLexicon.OLD_AGE_SEEDS),\n",
    "            \"OLD_AGE\", SeedSemanticLexicon.OLD_AGE_SEEDS, memory\n",
    "        )\n",
    "        self.promote(\n",
    "            self.mine_candidates(clauses, SeedSemanticLexicon.DISTRESS_SEEDS),\n",
    "            \"DISTRESS\", SeedSemanticLexicon.DISTRESS_SEEDS, memory\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ADAPTIVE SEMANTIC SIGNAL EXTRACTOR (STAGE 2.5)\n",
    "# ============================================================\n",
    "\n",
    "class LinguisticSegmenter:\n",
    "    SENTENCE_SPLIT_REGEX = r\"[.!?]\"\n",
    "    CLAUSE_SPLIT_REGEX = r\",|;| and | but | because | since \"\n",
    "\n",
    "    def split_sentences(self, text: str) -> List[str]:\n",
    "        return [\n",
    "            s.strip()\n",
    "            for s in re.split(self.SENTENCE_SPLIT_REGEX, text)\n",
    "            if s.strip()\n",
    "        ]\n",
    "\n",
    "    def split_clauses(self, sentence: str) -> List[str]:\n",
    "        return [\n",
    "            c.strip()\n",
    "            for c in re.split(self.CLAUSE_SPLIT_REGEX, sentence)\n",
    "            if c.strip()\n",
    "        ]\n",
    "\n",
    "\n",
    "class ContextAnalyzer:\n",
    "    NEGATION_TERMS = {\"not\", \"never\", \"no longer\", \"without\"}\n",
    "\n",
    "    def is_negated(self, text: str, phrase: str) -> bool:\n",
    "        idx = text.find(phrase)\n",
    "        if idx == -1:\n",
    "            return False\n",
    "        window = text[max(0, idx - 40):idx]\n",
    "        return any(n in window for n in self.NEGATION_TERMS)\n",
    "\n",
    "\n",
    "class SemanticSignal:\n",
    "    def __init__(self, signal_type: str, surface_form: str, sentence: str, clause: str):\n",
    "        self.signal_type = signal_type\n",
    "        self.surface_form = surface_form\n",
    "        self.sentence = sentence\n",
    "        self.clause = clause\n",
    "        self.base_weight = 1.0\n",
    "\n",
    "\n",
    "class SemanticEvidenceGraph:\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "        self.edges = defaultdict(list)\n",
    "\n",
    "    def add_signal(self, signal: SemanticSignal) -> int:\n",
    "        idx = len(self.nodes)\n",
    "        self.nodes.append(signal)\n",
    "        return idx\n",
    "\n",
    "    def auto_connect(self):\n",
    "        for i in range(len(self.nodes)):\n",
    "            for j in range(i + 1, len(self.nodes)):\n",
    "                if (\n",
    "                    self.nodes[i].sentence == self.nodes[j].sentence\n",
    "                    or self.nodes[i].clause == self.nodes[j].clause\n",
    "                ):\n",
    "                    self.edges[i].append(j)\n",
    "                    self.edges[j].append(i)\n",
    "\n",
    "\n",
    "class AdaptiveSemanticSignalExtractor:\n",
    "    \"\"\"\n",
    "    Deterministic, explainable semantic signal extraction engine.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.seed = SeedSemanticLexicon()\n",
    "        self.memory = LexiconMemory()\n",
    "        self.expander = StableLexiconExpansionEngine()\n",
    "        self.segmenter = LinguisticSegmenter()\n",
    "        self.context = ContextAnalyzer()\n",
    "\n",
    "    def adapt(self, text: str):\n",
    "        self.expander.update_from_text(text, self.segmenter, self.memory)\n",
    "\n",
    "    def extract_signals(self, text: str) -> SemanticEvidenceGraph:\n",
    "        graph = SemanticEvidenceGraph()\n",
    "\n",
    "        for sentence in self.segmenter.split_sentences(text):\n",
    "            for clause in self.segmenter.split_clauses(sentence):\n",
    "\n",
    "                for pattern in self.seed.AGE_PATTERNS:\n",
    "                    m = re.search(pattern, clause)\n",
    "                    if m:\n",
    "                        graph.add_signal(\n",
    "                            SemanticSignal(\"AGE\", m.group(0), sentence, clause)\n",
    "                        )\n",
    "\n",
    "                m = re.search(self.seed.LOCATION_PATTERN, clause)\n",
    "                if m:\n",
    "                    phrase = m.group(0).replace(\"live in\", \"\").strip()\n",
    "                    graph.add_signal(\n",
    "                        SemanticSignal(\"LOCATION\", phrase, sentence, clause)\n",
    "                    )\n",
    "\n",
    "                for seeds in self.seed.DISASTER_SEEDS.values():\n",
    "                    for term in seeds:\n",
    "                        if term in clause:\n",
    "                            graph.add_signal(\n",
    "                                SemanticSignal(\"DISASTER\", term, sentence, clause)\n",
    "                            )\n",
    "\n",
    "                for role, seeds in {\n",
    "                    \"WIDOW\": self.seed.WIDOW_SEEDS,\n",
    "                    \"DISABILITY\": self.seed.DISABILITY_SEEDS,\n",
    "                    \"OLD_AGE\": self.seed.OLD_AGE_SEEDS\n",
    "                }.items():\n",
    "                    for term in seeds:\n",
    "                        if term in clause and not self.context.is_negated(clause, term):\n",
    "                            graph.add_signal(\n",
    "                                SemanticSignal(role, term, sentence, clause)\n",
    "                            )\n",
    "\n",
    "                for term in self.seed.DISTRESS_SEEDS:\n",
    "                    if term in clause:\n",
    "                        graph.add_signal(\n",
    "                            SemanticSignal(\"DISTRESS\", term, sentence, clause)\n",
    "                        )\n",
    "\n",
    "        graph.auto_connect()\n",
    "        return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "04dfd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PART 3 — ULTRA-REFINED SEMANTIC REASONING ENGINE\n",
    "# ============================================================\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# ============================================================\n",
    "# 11. GRAPH ENERGY ANALYZER (EVIDENCE-WEIGHTED)\n",
    "# ============================================================\n",
    "\n",
    "class GraphEnergyAnalyzer:\n",
    "    \"\"\"\n",
    "    Computes semantic energy using:\n",
    "    - base signal weight\n",
    "    - connectivity\n",
    "    - evidence diversity (anti-noise)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: \"SemanticEvidenceGraph\"):\n",
    "        self.graph = graph\n",
    "\n",
    "    def signal_energy(self, idx: int) -> float:\n",
    "        node = self.graph.nodes[idx]\n",
    "        neighbors = self.graph.edges.get(idx, [])\n",
    "\n",
    "        # Penalize signals connected to many different roles\n",
    "        connected_roles = {\n",
    "            self.graph.nodes[j].signal_type for j in neighbors\n",
    "        }\n",
    "\n",
    "        diversity_penalty = 1 / max(len(connected_roles), 1)\n",
    "        connectivity = math.log1p(len(neighbors))\n",
    "\n",
    "        return node.base_weight * (0.6 + connectivity) * diversity_penalty\n",
    "\n",
    "    def role_energy(self, role: str) -> float:\n",
    "        energies = [\n",
    "            self.signal_energy(i)\n",
    "            for i, n in enumerate(self.graph.nodes)\n",
    "            if n.signal_type == role\n",
    "        ]\n",
    "\n",
    "        # Saturation control (diminishing returns)\n",
    "        return sum(energies) ** 0.85 if energies else 0.0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 12. PENSION INTENT RESOLUTION ENGINE (UNCERTAINTY-AWARE)\n",
    "# ============================================================\n",
    "\n",
    "class PensionIntentResolver:\n",
    "    \"\"\"\n",
    "    Resolves pension intent with explicit uncertainty handling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: \"SemanticEvidenceGraph\"):\n",
    "        self.energy = GraphEnergyAnalyzer(graph)\n",
    "\n",
    "    def resolve(self):\n",
    "        energies = {\n",
    "            \"Widow Pension\": self.energy.role_energy(\"WIDOW\"),\n",
    "            \"Disability Pension\": self.energy.role_energy(\"DISABILITY\"),\n",
    "            \"Old Age Pension\": self.energy.role_energy(\"OLD_AGE\")\n",
    "        }\n",
    "\n",
    "        if all(v == 0 for v in energies.values()):\n",
    "            return (\n",
    "                \"Unknown\",\n",
    "                0.3,\n",
    "                \"no qualifying vulnerability evidence\",\n",
    "                energies\n",
    "            )\n",
    "\n",
    "        ranked = sorted(\n",
    "            energies.items(), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "\n",
    "        top_role, top_energy = ranked[0]\n",
    "        second_energy = ranked[1][1]\n",
    "\n",
    "        dominance = top_energy - second_energy\n",
    "\n",
    "        # Ambiguity handling\n",
    "        if dominance < 0.25:\n",
    "            return (\n",
    "                top_role,\n",
    "                0.55,\n",
    "                \"competing vulnerability evidence detected\",\n",
    "                energies\n",
    "            )\n",
    "\n",
    "        confidence = min(0.65 + dominance, 0.95)\n",
    "\n",
    "        return (\n",
    "            top_role,\n",
    "            confidence,\n",
    "            \"resolved via dominant semantic evidence\",\n",
    "            energies\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 13. DISASTER EVENT RESOLUTION ENGINE (CONSENSUS-BASED)\n",
    "# ============================================================\n",
    "\n",
    "class DisasterResolver:\n",
    "    \"\"\"\n",
    "    Resolves disaster event using\n",
    "    energy + consensus reinforcement.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: \"SemanticEvidenceGraph\"):\n",
    "        self.graph = graph\n",
    "        self.energy = GraphEnergyAnalyzer(graph)\n",
    "\n",
    "    def resolve(self):\n",
    "        scores = defaultdict(list)\n",
    "\n",
    "        for i, node in enumerate(self.graph.nodes):\n",
    "            if node.signal_type == \"DISASTER\":\n",
    "                scores[node.surface_form].append(\n",
    "                    self.energy.signal_energy(i)\n",
    "                )\n",
    "\n",
    "        if not scores:\n",
    "            return \"Unknown\", 0.0, \"no disaster evidence\"\n",
    "\n",
    "        phrase_scores = {\n",
    "            k: sum(v) * (1 + math.log1p(len(v)))\n",
    "            for k, v in scores.items()\n",
    "        }\n",
    "\n",
    "        best = max(phrase_scores, key=phrase_scores.get)\n",
    "        strength = phrase_scores[best]\n",
    "\n",
    "        confidence = min(0.7 + 0.15 * strength, 0.95)\n",
    "\n",
    "        return best, confidence, \"resolved by reinforced disaster consensus\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 14. ATTRIBUTE RESOLVER (ROBUST AGGREGATION)\n",
    "# ============================================================\n",
    "\n",
    "class AttributeResolver:\n",
    "    \"\"\"\n",
    "    Resolves factual attributes using redundancy tolerance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: \"SemanticEvidenceGraph\"):\n",
    "        self.graph = graph\n",
    "\n",
    "    def resolve_age(self):\n",
    "        ages = []\n",
    "\n",
    "        for n in self.graph.nodes:\n",
    "            if n.signal_type == \"AGE\":\n",
    "                m = re.search(r\"\\d{2}\", n.surface_form)\n",
    "                if m:\n",
    "                    ages.append(int(m.group()))\n",
    "\n",
    "        if not ages:\n",
    "            return \"Unknown\", 0.0, \"age not mentioned\"\n",
    "\n",
    "        age = round(sum(ages) / len(ages))\n",
    "        confidence = min(0.9 + 0.02 * len(ages), 0.97)\n",
    "\n",
    "        return age, confidence, \"aggregated age evidence\"\n",
    "\n",
    "    def resolve_location(self):\n",
    "        locations = [\n",
    "            n.surface_form for n in self.graph.nodes\n",
    "            if n.signal_type == \"LOCATION\"\n",
    "        ]\n",
    "\n",
    "        if not locations:\n",
    "            return \"Unknown\", 0.0, \"location not mentioned\"\n",
    "\n",
    "        return locations[0], 0.85, \"explicit location phrase\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 15. DISTRESS SEVERITY ANALYZER (EVIDENCE-DENSITY BASED)\n",
    "# ============================================================\n",
    "\n",
    "class DistressSeverityAnalyzer:\n",
    "    \"\"\"\n",
    "    Computes distress severity using\n",
    "    density and reinforcement.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: \"SemanticEvidenceGraph\"):\n",
    "        self.energy = GraphEnergyAnalyzer(graph)\n",
    "        self.graph = graph\n",
    "\n",
    "    def severity_score(self):\n",
    "        energies = [\n",
    "            self.energy.signal_energy(i)\n",
    "            for i, n in enumerate(self.graph.nodes)\n",
    "            if n.signal_type == \"DISTRESS\"\n",
    "        ]\n",
    "\n",
    "        if not energies:\n",
    "            return 0.25\n",
    "\n",
    "        severity = 0.3 + 0.2 * math.log1p(sum(energies))\n",
    "        return min(severity, 1.0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 16. EXPLAINABILITY ENGINE (DECISION-CENTRIC)\n",
    "# ============================================================\n",
    "\n",
    "class ExplainabilityEngine:\n",
    "    \"\"\"\n",
    "    Generates decision-aligned explanations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: \"SemanticEvidenceGraph\"):\n",
    "        self.graph = graph\n",
    "\n",
    "    def evidence_by_role(self, role: str):\n",
    "        return [\n",
    "            n.surface_form for n in self.graph.nodes\n",
    "            if n.signal_type == role\n",
    "        ]\n",
    "\n",
    "    def full_trace(self):\n",
    "        return {\n",
    "            role: self.evidence_by_role(role)\n",
    "            for role in SEMANTIC_ROLES\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 17. FULL SEMANTIC DECISION ENGINE (FINAL)\n",
    "# ============================================================\n",
    "\n",
    "class SemanticDecisionEngine:\n",
    "    \"\"\"\n",
    "    End-to-end semantic reasoning engine.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: \"SemanticEvidenceGraph\"):\n",
    "        self.graph = graph\n",
    "        self.intent = PensionIntentResolver(graph)\n",
    "        self.disaster = DisasterResolver(graph)\n",
    "        self.attributes = AttributeResolver(graph)\n",
    "        self.distress = DistressSeverityAnalyzer(graph)\n",
    "        self.explainer = ExplainabilityEngine(graph)\n",
    "\n",
    "    def decide(self) -> dict:\n",
    "        age, age_c, age_r = self.attributes.resolve_age()\n",
    "        loc, loc_c, loc_r = self.attributes.resolve_location()\n",
    "\n",
    "        pension, pen_c, pen_r, pen_debug = self.intent.resolve()\n",
    "        disaster, dis_c, dis_r = self.disaster.resolve()\n",
    "        severity = self.distress.severity_score()\n",
    "\n",
    "        return {\n",
    "            \"Age\": {\"value\": age, \"confidence\": age_c, \"reason\": age_r},\n",
    "            \"Location\": {\"value\": loc, \"confidence\": loc_c, \"reason\": loc_r},\n",
    "            \"Pension_Type\": {\n",
    "                \"value\": pension,\n",
    "                \"confidence\": pen_c,\n",
    "                \"reason\": pen_r,\n",
    "                \"debug_energy\": pen_debug\n",
    "            },\n",
    "            \"Disaster_Event\": {\n",
    "                \"value\": disaster,\n",
    "                \"confidence\": dis_c,\n",
    "                \"reason\": dis_r\n",
    "            },\n",
    "            \"Disaster_Severity\": severity,\n",
    "            \"Explainability_Trace\": self.explainer.full_trace()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ae70a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SEER-NLP model saved to: seer_nlp_model.pkl\n",
      "✅ SEER-NLP model loaded from: seer_nlp_model.pkl\n",
      "{\n",
      "    \"Age\": {\n",
      "        \"value\": 72,\n",
      "        \"confidence\": 0.9400000000000001,\n",
      "        \"reason\": \"aggregated age evidence\"\n",
      "    },\n",
      "    \"Location\": {\n",
      "        \"value\": \"gaya district\",\n",
      "        \"confidence\": 0.85,\n",
      "        \"reason\": \"explicit location phrase\"\n",
      "    },\n",
      "    \"Pension_Type\": {\n",
      "        \"value\": \"Widow Pension\",\n",
      "        \"confidence\": 0.95,\n",
      "        \"reason\": \"resolved via dominant semantic evidence\",\n",
      "        \"debug_energy\": {\n",
      "            \"Widow Pension\": 0.9941721018101672,\n",
      "            \"Disability Pension\": 0.0,\n",
      "            \"Old Age Pension\": 0.0\n",
      "        }\n",
      "    },\n",
      "    \"Disaster_Event\": {\n",
      "        \"value\": \"flood\",\n",
      "        \"confidence\": 0.868154434864613,\n",
      "        \"reason\": \"resolved by reinforced disaster consensus\"\n",
      "    },\n",
      "    \"Disaster_Severity\": 0.4379429774509145,\n",
      "    \"Explainability_Trace\": {\n",
      "        \"OLD_AGE\": [],\n",
      "        \"DISTRESS\": [\n",
      "            \"no income\"\n",
      "        ],\n",
      "        \"DISASTER\": [\n",
      "            \"flood\",\n",
      "            \"floods\"\n",
      "        ],\n",
      "        \"DISABILITY\": [],\n",
      "        \"AGE\": [\n",
      "            \"72 years old\",\n",
      "            \"i am 72 years\"\n",
      "        ],\n",
      "        \"WIDOW\": [\n",
      "            \"lost my husband\"\n",
      "        ],\n",
      "        \"LOCATION\": [\n",
      "            \"gaya district\"\n",
      "        ]\n",
      "    },\n",
      "    \"_meta\": {\n",
      "        \"model_version\": \"SEER-NLP v1.1\",\n",
      "        \"schema_version\": \"1.1\",\n",
      "        \"model_fingerprint\": \"8238ca4d24eb\",\n",
      "        \"adaptation_used\": false,\n",
      "        \"normalization_trace\": [\n",
      "            \"unicode normalized\",\n",
      "            \"lowercased\",\n",
      "            \"canonicalized \\\\bno income\\\\b\",\n",
      "            \"normalized age expressions\",\n",
      "            \"normalized whitespace\"\n",
      "        ],\n",
      "        \"noise_score\": 0.0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PART 4 — SEER-NLP MASTER MODEL & DEPLOYMENT LAYER (FINAL)\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "import hashlib\n",
    "from typing import Iterable, List, Dict\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 18. SEER-NLP MASTER MODEL (FULL PIPELINE)\n",
    "# ============================================================\n",
    "\n",
    "class SEERNLPModel:\n",
    "    \"\"\"\n",
    "    SEER-NLP Master Model\n",
    "\n",
    "    Orchestrates:\n",
    "    - Text normalization\n",
    "    - Adaptive semantic extraction\n",
    "    - Evidence graph construction\n",
    "    - Semantic reasoning\n",
    "    - Explainability generation\n",
    "\n",
    "    This object is serialized for deployment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Core NLP components\n",
    "        self.normalizer = TextNormalizer()\n",
    "        self.signal_extractor = AdaptiveSemanticSignalExtractor()\n",
    "\n",
    "        # Metadata\n",
    "        self.version = \"SEER-NLP v1.1\"\n",
    "        self.schema_version = \"1.1\"\n",
    "        self.description = (\n",
    "            \"Semantic Explainable Extraction & Reasoning \"\n",
    "            \"for Pension Applications in Disaster Response\"\n",
    "        )\n",
    "\n",
    "        self.model_fingerprint = self._build_fingerprint()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # MODEL FINGERPRINT (REPRODUCIBILITY)\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def _build_fingerprint(self) -> str:\n",
    "        signature = (\n",
    "            self.version +\n",
    "            self.schema_version +\n",
    "            self.description\n",
    "        )\n",
    "        return hashlib.sha256(signature.encode()).hexdigest()[:12]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # SINGLE DOCUMENT INFERENCE\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def infer(self, raw_text: str, adapt: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform full semantic extraction & reasoning\n",
    "        on a single application narrative.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Normalize\n",
    "        norm = self.normalizer.normalize(raw_text)\n",
    "        text = norm[\"normalized_text\"]\n",
    "\n",
    "        # Step 2: Optional controlled adaptation\n",
    "        if adapt:\n",
    "            self.signal_extractor.adapt(text)\n",
    "\n",
    "        # Step 3: Semantic extraction\n",
    "        graph = self.signal_extractor.extract_signals(text)\n",
    "\n",
    "        # Step 4: Semantic reasoning\n",
    "        decision_engine = SemanticDecisionEngine(graph)\n",
    "        result = decision_engine.decide()\n",
    "\n",
    "        # Step 5: Attach governance metadata\n",
    "        result[\"_meta\"] = {\n",
    "            \"model_version\": self.version,\n",
    "            \"schema_version\": self.schema_version,\n",
    "            \"model_fingerprint\": self.model_fingerprint,\n",
    "            \"adaptation_used\": adapt,\n",
    "            \"normalization_trace\": norm[\"normalization_trace\"],\n",
    "            \"noise_score\": norm[\"noise_score\"]\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # BATCH INFERENCE\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def infer_batch(\n",
    "        self,\n",
    "        texts: Iterable[str],\n",
    "        adapt: bool = False\n",
    "    ) -> List[Dict]:\n",
    "\n",
    "        return [self.infer(t, adapt=adapt) for t in texts]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 19. MODEL SERIALIZATION & LOADING\n",
    "# ============================================================\n",
    "\n",
    "class ModelPersistence:\n",
    "\n",
    "    @staticmethod\n",
    "    def save(model: SEERNLPModel, path: str):\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"✅ SEER-NLP model saved to: {path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str) -> SEERNLPModel:\n",
    "        with open(path, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"✅ SEER-NLP model loaded from: {path}\")\n",
    "        return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 20. SERVICE WRAPPER (STREAMLIT / API)\n",
    "# ============================================================\n",
    "\n",
    "class SEERNLPService:\n",
    "\n",
    "    def __init__(self, model_path: str):\n",
    "        self.model = ModelPersistence.load(model_path)\n",
    "\n",
    "    def process_text(self, text: str) -> Dict:\n",
    "        return self.model.infer(text, adapt=False)\n",
    "\n",
    "    def process_text_with_adaptation(self, text: str) -> Dict:\n",
    "        return self.model.infer(text, adapt=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 21. ONE-TIME MODEL BUILD SCRIPT\n",
    "# ============================================================\n",
    "\n",
    "def build_and_save_seer_model(\n",
    "    output_path: str = \"seer_nlp_model.pkl\"\n",
    "):\n",
    "    model = SEERNLPModel()\n",
    "    ModelPersistence.save(model, output_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 22. EXAMPLE USAGE\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sample_text = \"\"\"\n",
    "    I am 72 years old and live in Gaya district.\n",
    "    After floods damaged my house, I lost my husband\n",
    "    and now have no income to survive.\n",
    "    \"\"\"\n",
    "\n",
    "    build_and_save_seer_model(\"seer_nlp_model.pkl\")\n",
    "\n",
    "    service = SEERNLPService(\"seer_nlp_model.pkl\")\n",
    "    output = service.process_text(sample_text)\n",
    "\n",
    "    import json\n",
    "    print(json.dumps(output, indent=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
