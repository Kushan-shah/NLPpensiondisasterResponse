{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f613bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SEER-NLP\n",
    "Semantic Explainable Extraction & Reasoning\n",
    "for Pension Applications in Disaster Response\n",
    "\n",
    "Author: (Your Name)\n",
    "Purpose: Research-grade NLP framework\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# ============================================================\n",
    "# 1. TEXT NORMALIZATION ENGINE\n",
    "# ============================================================\n",
    "\n",
    "class TextNormalizer:\n",
    "    \"\"\"\n",
    "    Responsible for linguistic normalization.\n",
    "    This is intentionally separated for extensibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.replacements = {\n",
    "            \"yrs\": \"years\",\n",
    "            \"yr\": \"year\",\n",
    "            \"govt\": \"government\"\n",
    "        }\n",
    "\n",
    "    def normalize(self, text: str) -> str:\n",
    "        text = text.lower()\n",
    "        for k, v in self.replacements.items():\n",
    "            text = text.replace(k, v)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "# ============================================================\n",
    "# 2. SENTENCE & CLAUSE INTELLIGENCE\n",
    "# ============================================================\n",
    "\n",
    "class LinguisticSegmenter:\n",
    "    \"\"\"\n",
    "    Performs sentence and clause-level segmentation.\n",
    "    Clause segmentation adds novelty and depth.\n",
    "    \"\"\"\n",
    "\n",
    "    SENTENCE_SPLIT_REGEX = r\"[.!?]\"\n",
    "    CLAUSE_SPLIT_REGEX = r\",|;| and | but | because | since \"\n",
    "\n",
    "    def split_sentences(self, text: str) -> List[str]:\n",
    "        return [s.strip() for s in re.split(self.SENTENCE_SPLIT_REGEX, text) if s.strip()]\n",
    "\n",
    "    def split_clauses(self, sentence: str) -> List[str]:\n",
    "        return [c.strip() for c in re.split(self.CLAUSE_SPLIT_REGEX, sentence) if c.strip()]\n",
    "\n",
    "# ============================================================\n",
    "# 3. SEMANTIC SIGNAL PRIMITIVE (CORE CONCEPT)\n",
    "# ============================================================\n",
    "\n",
    "class SemanticSignal:\n",
    "    \"\"\"\n",
    "    Atomic unit of meaning extracted from text.\n",
    "    Signals are later connected into a graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        signal_type: str,\n",
    "        surface_form: str,\n",
    "        sentence: str,\n",
    "        clause: str\n",
    "    ):\n",
    "        self.signal_type = signal_type\n",
    "        self.surface_form = surface_form\n",
    "        self.sentence = sentence\n",
    "        self.clause = clause\n",
    "        self.base_weight = self.assign_weight()\n",
    "\n",
    "    def assign_weight(self) -> float:\n",
    "        \"\"\"\n",
    "        Base semantic importance by signal type.\n",
    "        \"\"\"\n",
    "        weights = {\n",
    "            \"AGE\": 0.9,\n",
    "            \"LOCATION\": 0.7,\n",
    "            \"DISASTER\": 0.9,\n",
    "            \"WIDOW\": 1.0,\n",
    "            \"DISABILITY\": 1.0,\n",
    "            \"OLD_AGE\": 0.8,\n",
    "            \"DISTRESS\": 0.6\n",
    "        }\n",
    "        return weights.get(self.signal_type, 0.5)\n",
    "\n",
    "# ============================================================\n",
    "# 4. CONTEXT & NEGATION ANALYZER\n",
    "# ============================================================\n",
    "\n",
    "class ContextAnalyzer:\n",
    "    \"\"\"\n",
    "    Handles negation and contextual weakening.\n",
    "    \"\"\"\n",
    "\n",
    "    NEGATION_TERMS = [\n",
    "        \"not\", \"never\", \"no longer\", \"without\", \"none\"\n",
    "    ]\n",
    "\n",
    "    def is_negated(self, text: str, phrase: str) -> bool:\n",
    "        idx = text.find(phrase)\n",
    "        if idx == -1:\n",
    "            return False\n",
    "        window = text[max(0, idx - 35):idx]\n",
    "        return any(n in window for n in self.NEGATION_TERMS)\n",
    "\n",
    "# ============================================================\n",
    "# 5. SEMANTIC EVIDENCE GRAPH (KEY NOVELTY)\n",
    "# ============================================================\n",
    "\n",
    "class SemanticEvidenceGraph:\n",
    "    \"\"\"\n",
    "    Graph-based semantic reasoning structure.\n",
    "\n",
    "    Nodes   → Semantic Signals\n",
    "    Edges   → Contextual co-occurrence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nodes: List[SemanticSignal] = []\n",
    "        self.edges: Dict[int, List[int]] = defaultdict(list)\n",
    "\n",
    "    def add_signal(self, signal: SemanticSignal) -> int:\n",
    "        idx = len(self.nodes)\n",
    "        self.nodes.append(signal)\n",
    "        return idx\n",
    "\n",
    "    def connect(self, i: int, j: int):\n",
    "        if j not in self.edges[i]:\n",
    "            self.edges[i].append(j)\n",
    "        if i not in self.edges[j]:\n",
    "            self.edges[j].append(i)\n",
    "\n",
    "    def auto_connect(self):\n",
    "        \"\"\"\n",
    "        Connect signals appearing in same sentence or clause.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.nodes)):\n",
    "            for j in range(i + 1, len(self.nodes)):\n",
    "                if (\n",
    "                    self.nodes[i].sentence == self.nodes[j].sentence\n",
    "                    or self.nodes[i].clause == self.nodes[j].clause\n",
    "                ):\n",
    "                    self.connect(i, j)\n",
    "\n",
    "    def score_signal_type(self, signal_type: str) -> float:\n",
    "        score = 0.0\n",
    "        for node in self.nodes:\n",
    "            if node.signal_type == signal_type:\n",
    "                score += node.base_weight\n",
    "        return score\n",
    "\n",
    "    def collect_evidence(self, signal_type: str) -> List[str]:\n",
    "        return [\n",
    "            n.surface_form for n in self.nodes\n",
    "            if n.signal_type == signal_type\n",
    "        ]\n",
    "# ============================================================\n",
    "# PART 2 — ADAPTIVE SEMANTIC LEXICON SYSTEM (RESEARCH CORE)\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List\n",
    "\n",
    "# ============================================================\n",
    "# 6. SEMANTIC ROLE DEFINITIONS (EXPLICIT ONTOLOGY)\n",
    "# ============================================================\n",
    "\n",
    "SEMANTIC_ROLES = [\n",
    "    \"AGE\",\n",
    "    \"LOCATION\",\n",
    "    \"DISASTER\",\n",
    "    \"WIDOW\",\n",
    "    \"DISABILITY\",\n",
    "    \"OLD_AGE\",\n",
    "    \"DISTRESS\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 7. SEED SEMANTIC LEXICON (HIGH PRECISION, LOW RECALL)\n",
    "# ============================================================\n",
    "\n",
    "class SeedSemanticLexicon:\n",
    "    \"\"\"\n",
    "    Seed lexicon = semantic anchors.\n",
    "    These phrases are trusted and NEVER auto-removed.\n",
    "    \"\"\"\n",
    "\n",
    "    AGE_PATTERNS = [\n",
    "        r\"\\b(\\d{2})\\s+year[s]?\\s+old\\b\",\n",
    "        r\"\\bat\\s+the\\s+age\\s+of\\s+(\\d{2})\\b\",\n",
    "        r\"\\baged\\s+(\\d{2})\\b\",\n",
    "        r\"\\bi\\s+am\\s+(\\d{2})\\s+years\\b\",\n",
    "        r\"\\bmy\\s+age\\s+is\\s+(\\d{2})\\b\"\n",
    "    ]\n",
    "\n",
    "    LOCATION_PATTERN = r\"(district|city|village)\\s+([a-z ]+)\"\n",
    "\n",
    "    DISASTER_SEEDS = {\n",
    "        \"Flood\": [\"flood\", \"floods\", \"flooding\"],\n",
    "        \"Cyclone\": [\"cyclone\", \"storm\", \"cyclonic\"],\n",
    "        \"Fire\": [\"fire\", \"burnt\", \"burned\"],\n",
    "        \"Earthquake\": [\"earthquake\", \"tremor\", \"seismic\"]\n",
    "    }\n",
    "\n",
    "    WIDOW_SEEDS = [\n",
    "        \"widow\", \"widowed\",\n",
    "        \"husband passed away\",\n",
    "        \"lost my husband\",\n",
    "        \"after my husband died\"\n",
    "    ]\n",
    "\n",
    "    DISABILITY_SEEDS = [\n",
    "        \"disabled\",\n",
    "        \"unable to work\",\n",
    "        \"physically impaired\",\n",
    "        \"medically unfit\",\n",
    "        \"chronic illness\"\n",
    "    ]\n",
    "\n",
    "    OLD_AGE_SEEDS = [\n",
    "        \"too old to work\",\n",
    "        \"because of my age\",\n",
    "        \"elderly\",\n",
    "        \"advanced age\",\n",
    "        \"no strength to work\"\n",
    "    ]\n",
    "\n",
    "    DISTRESS_SEEDS = [\n",
    "        \"no income\",\n",
    "        \"lost everything\",\n",
    "        \"nowhere to go\",\n",
    "        \"urgent help\",\n",
    "        \"dependent on support\"\n",
    "    ]\n",
    "\n",
    "# ============================================================\n",
    "# 8. LEXICON MEMORY (PERSISTENT & SERIALIZABLE)\n",
    "# ============================================================\n",
    "\n",
    "class LexiconMemory:\n",
    "    \"\"\"\n",
    "    Stores learned lexicon expansions.\n",
    "    This object is saved inside the pickle model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.role_phrases = {\n",
    "            \"WIDOW\": set(),\n",
    "            \"DISABILITY\": set(),\n",
    "            \"OLD_AGE\": set(),\n",
    "            \"DISTRESS\": set()\n",
    "        }\n",
    "\n",
    "    def add(self, role: str, phrase: str):\n",
    "        self.role_phrases[role].add(phrase)\n",
    "\n",
    "    def get(self, role: str):\n",
    "        return self.role_phrases.get(role, set())\n",
    "\n",
    "# ============================================================\n",
    "# 9. SEMANTIC STABILITY–AWARE LEXICON EXPANSION ENGINE (NOVEL)\n",
    "# ============================================================\n",
    "\n",
    "class StableLexiconExpansionEngine:\n",
    "    \"\"\"\n",
    "    Discovers new semantic phrases using:\n",
    "    - frequency\n",
    "    - role purity\n",
    "    - contextual stability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_freq=3, min_role_purity=0.7):\n",
    "        self.min_freq = min_freq\n",
    "        self.min_role_purity = min_role_purity\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # N-GRAM CANDIDATE MINING\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def mine_candidates(self, clauses: List[str], seed_phrases: List[str]):\n",
    "        candidates = defaultdict(list)\n",
    "\n",
    "        for clause in clauses:\n",
    "            if any(seed in clause for seed in seed_phrases):\n",
    "                tokens = clause.split()\n",
    "                for n in [2, 3, 4]:\n",
    "                    for gram in zip(*[tokens[i:] for i in range(n)]):\n",
    "                        phrase = \" \".join(gram)\n",
    "                        if phrase not in seed_phrases:\n",
    "                            candidates[phrase].append(clause)\n",
    "\n",
    "        return candidates\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # ROLE PURITY EVALUATION\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def role_purity(self, contexts, role_seeds):\n",
    "        hits = sum(\n",
    "            1 for c in contexts\n",
    "            if any(seed in c for seed in role_seeds)\n",
    "        )\n",
    "        return hits / len(contexts)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # PROMOTION DECISION\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def promote(self, candidates, role, role_seeds, memory: LexiconMemory):\n",
    "        for phrase, contexts in candidates.items():\n",
    "            if len(contexts) < self.min_freq:\n",
    "                continue\n",
    "\n",
    "            purity = self.role_purity(contexts, role_seeds)\n",
    "            if purity >= self.min_role_purity:\n",
    "                memory.add(role, phrase)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # FULL UPDATE PIPELINE\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def update_from_text(self, text, segmenter, memory: LexiconMemory):\n",
    "        sentences = segmenter.split_sentences(text)\n",
    "        clauses = []\n",
    "        for s in sentences:\n",
    "            clauses.extend(segmenter.split_clauses(s))\n",
    "\n",
    "        self.promote(\n",
    "            self.mine_candidates(clauses, SeedSemanticLexicon.WIDOW_SEEDS),\n",
    "            \"WIDOW\", SeedSemanticLexicon.WIDOW_SEEDS, memory\n",
    "        )\n",
    "\n",
    "        self.promote(\n",
    "            self.mine_candidates(clauses, SeedSemanticLexicon.DISABILITY_SEEDS),\n",
    "            \"DISABILITY\", SeedSemanticLexicon.DISABILITY_SEEDS, memory\n",
    "        )\n",
    "\n",
    "        self.promote(\n",
    "            self.mine_candidates(clauses, SeedSemanticLexicon.OLD_AGE_SEEDS),\n",
    "            \"OLD_AGE\", SeedSemanticLexicon.OLD_AGE_SEEDS, memory\n",
    "        )\n",
    "\n",
    "        self.promote(\n",
    "            self.mine_candidates(clauses, SeedSemanticLexicon.DISTRESS_SEEDS),\n",
    "            \"DISTRESS\", SeedSemanticLexicon.DISTRESS_SEEDS, memory\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# 10. ADAPTIVE SEMANTIC SIGNAL EXTRACTOR (CORE NLP ENGINE)\n",
    "# ============================================================\n",
    "\n",
    "class AdaptiveSemanticSignalExtractor:\n",
    "    \"\"\"\n",
    "    Extracts semantic signals using:\n",
    "    - seed lexicons (precision)\n",
    "    - expanded lexicons (recall)\n",
    "    - negation-aware context analysis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.seed = SeedSemanticLexicon()\n",
    "        self.memory = LexiconMemory()\n",
    "        self.expander = StableLexiconExpansionEngine()\n",
    "        self.segmenter = LinguisticSegmenter()\n",
    "        self.context = ContextAnalyzer()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # ONLINE ADAPTATION HOOK (OPTIONAL)\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def adapt(self, text: str):\n",
    "        self.expander.update_from_text(\n",
    "            text, self.segmenter, self.memory\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # SEMANTIC SIGNAL EXTRACTION\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def extract_signals(self, text: str) -> SemanticEvidenceGraph:\n",
    "        graph = SemanticEvidenceGraph()\n",
    "\n",
    "        sentences = self.segmenter.split_sentences(text)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            clauses = self.segmenter.split_clauses(sentence)\n",
    "\n",
    "            for clause in clauses:\n",
    "                self._age(clause, sentence, graph)\n",
    "                self._location(clause, sentence, graph)\n",
    "                self._disaster(clause, sentence, graph)\n",
    "                self._vulnerability(clause, sentence, graph)\n",
    "                self._distress(clause, sentence, graph)\n",
    "\n",
    "        graph.auto_connect()\n",
    "        return graph\n",
    "\n",
    "    # ---------------- SIGNAL HELPERS ----------------\n",
    "\n",
    "    def _age(self, clause, sentence, graph):\n",
    "        for p in self.seed.AGE_PATTERNS:\n",
    "            m = re.search(p, clause)\n",
    "            if m:\n",
    "                graph.add_signal(\n",
    "                    SemanticSignal(\"AGE\", m.group(0), sentence, clause)\n",
    "                )\n",
    "\n",
    "    def _location(self, clause, sentence, graph):\n",
    "        m = re.search(self.seed.LOCATION_PATTERN, clause)\n",
    "        if m:\n",
    "            graph.add_signal(\n",
    "                SemanticSignal(\"LOCATION\", m.group(0), sentence, clause)\n",
    "            )\n",
    "\n",
    "    def _disaster(self, clause, sentence, graph):\n",
    "        for _, seeds in self.seed.DISASTER_SEEDS.items():\n",
    "            for s in seeds:\n",
    "                if s in clause:\n",
    "                    graph.add_signal(\n",
    "                        SemanticSignal(\"DISASTER\", s, sentence, clause)\n",
    "                    )\n",
    "\n",
    "    def _vulnerability(self, clause, sentence, graph):\n",
    "        for role, seeds in {\n",
    "            \"WIDOW\": self.seed.WIDOW_SEEDS,\n",
    "            \"DISABILITY\": self.seed.DISABILITY_SEEDS,\n",
    "            \"OLD_AGE\": self.seed.OLD_AGE_SEEDS\n",
    "        }.items():\n",
    "            for s in seeds:\n",
    "                if s in clause and not self.context.is_negated(clause, s):\n",
    "                    graph.add_signal(\n",
    "                        SemanticSignal(role, s, sentence, clause)\n",
    "                    )\n",
    "\n",
    "        # Expanded lexicon usage\n",
    "        for role in [\"WIDOW\", \"DISABILITY\", \"OLD_AGE\"]:\n",
    "            for phrase in self.memory.get(role):\n",
    "                if phrase in clause:\n",
    "                    graph.add_signal(\n",
    "                        SemanticSignal(role, phrase, sentence, clause)\n",
    "                    )\n",
    "\n",
    "    def _distress(self, clause, sentence, graph):\n",
    "        for s in self.seed.DISTRESS_SEEDS:\n",
    "            if s in clause:\n",
    "                graph.add_signal(\n",
    "                    SemanticSignal(\"DISTRESS\", s, sentence, clause)\n",
    "                )\n",
    "\n",
    "        for s in self.memory.get(\"DISTRESS\"):\n",
    "            if s in clause:\n",
    "                graph.add_signal(\n",
    "                    SemanticSignal(\"DISTRESS\", s, sentence, clause)\n",
    "                )\n",
    "# ============================================================\n",
    "# PART 3 — SEMANTIC REASONING ENGINE (CORE NOVELTY)\n",
    "# ============================================================\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# ============================================================\n",
    "# 11. GRAPH ENERGY & SIGNAL CENTRALITY ANALYZER\n",
    "# ============================================================\n",
    "\n",
    "class GraphEnergyAnalyzer:\n",
    "    \"\"\"\n",
    "    Computes semantic strength using:\n",
    "    - signal weights\n",
    "    - graph connectivity\n",
    "    - role concentration\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: SemanticEvidenceGraph):\n",
    "        self.graph = graph\n",
    "\n",
    "    def signal_energy(self, idx: int) -> float:\n",
    "        \"\"\"\n",
    "        Energy of a signal = base weight × (1 + degree)\n",
    "        \"\"\"\n",
    "        node = self.graph.nodes[idx]\n",
    "        degree = len(self.graph.edges.get(idx, []))\n",
    "        return node.base_weight * (1 + math.log1p(degree))\n",
    "\n",
    "    def role_energy(self, role: str) -> float:\n",
    "        \"\"\"\n",
    "        Total semantic energy of a role\n",
    "        \"\"\"\n",
    "        energy = 0.0\n",
    "        for i, node in enumerate(self.graph.nodes):\n",
    "            if node.signal_type == role:\n",
    "                energy += self.signal_energy(i)\n",
    "        return energy\n",
    "\n",
    "    def role_distribution(self) -> dict:\n",
    "        \"\"\"\n",
    "        Normalized role energy distribution\n",
    "        \"\"\"\n",
    "        role_scores = defaultdict(float)\n",
    "        for role in SEMANTIC_ROLES:\n",
    "            role_scores[role] = self.role_energy(role)\n",
    "\n",
    "        total = sum(role_scores.values()) or 1.0\n",
    "        return {r: v / total for r, v in role_scores.items()}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 12. PENSION INTENT RESOLUTION ENGINE (CONFLICT AWARE)\n",
    "# ============================================================\n",
    "\n",
    "class PensionIntentResolver:\n",
    "    \"\"\"\n",
    "    Resolves pension intent using:\n",
    "    - role energy\n",
    "    - mutual exclusion\n",
    "    - dominance margins\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: SemanticEvidenceGraph):\n",
    "        self.energy = GraphEnergyAnalyzer(graph)\n",
    "\n",
    "    def resolve(self):\n",
    "        energies = {\n",
    "            \"Widow Pension\": self.energy.role_energy(\"WIDOW\"),\n",
    "            \"Disability Pension\": self.energy.role_energy(\"DISABILITY\"),\n",
    "            \"Old Age Pension\": self.energy.role_energy(\"OLD_AGE\")\n",
    "        }\n",
    "\n",
    "        # No signals found\n",
    "        if all(v == 0 for v in energies.values()):\n",
    "            return \"Unknown\", 0.3, \"no vulnerability signals detected\"\n",
    "\n",
    "        # Sort by strength\n",
    "        sorted_roles = sorted(\n",
    "            energies.items(), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "\n",
    "        top_role, top_score = sorted_roles[0]\n",
    "        second_score = sorted_roles[1][1]\n",
    "\n",
    "        # Confidence based on dominance gap\n",
    "        dominance = top_score - second_score\n",
    "        confidence = min(0.6 + dominance, 0.95)\n",
    "\n",
    "        reason = (\n",
    "            f\"resolved by semantic dominance: \"\n",
    "            f\"{top_role} ({top_score:.2f}) > others\"\n",
    "        )\n",
    "\n",
    "        return top_role, confidence, reason\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 13. DISASTER EVENT RESOLUTION ENGINE\n",
    "# ============================================================\n",
    "\n",
    "class DisasterResolver:\n",
    "    \"\"\"\n",
    "    Determines disaster event using graph energy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: SemanticEvidenceGraph):\n",
    "        self.graph = graph\n",
    "        self.energy = GraphEnergyAnalyzer(graph)\n",
    "\n",
    "    def resolve(self):\n",
    "        disaster_energy = defaultdict(float)\n",
    "\n",
    "        for i, node in enumerate(self.graph.nodes):\n",
    "            if node.signal_type == \"DISASTER\":\n",
    "                disaster_energy[node.surface_form] += (\n",
    "                    self.energy.signal_energy(i)\n",
    "                )\n",
    "\n",
    "        if not disaster_energy:\n",
    "            return \"Unknown\", 0.0, \"no disaster signals\"\n",
    "\n",
    "        # Pick strongest signal cluster\n",
    "        best_phrase = max(disaster_energy, key=disaster_energy.get)\n",
    "        confidence = min(0.8 + disaster_energy[best_phrase], 0.95)\n",
    "\n",
    "        return best_phrase, confidence, \"disaster energy dominance\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 14. AGE & LOCATION REFINEMENT ENGINE\n",
    "# ============================================================\n",
    "\n",
    "class AttributeResolver:\n",
    "    \"\"\"\n",
    "    Resolves factual attributes (age, location)\n",
    "    with redundancy handling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: SemanticEvidenceGraph):\n",
    "        self.graph = graph\n",
    "\n",
    "    def resolve_age(self):\n",
    "        for node in self.graph.nodes:\n",
    "            if node.signal_type == \"AGE\":\n",
    "                age = int(re.search(r\"\\d{2}\", node.surface_form).group())\n",
    "                return age, 0.95, \"explicit age detected\"\n",
    "        return \"Unknown\", 0.0, \"age not found\"\n",
    "\n",
    "    def resolve_location(self):\n",
    "        for node in self.graph.nodes:\n",
    "            if node.signal_type == \"LOCATION\":\n",
    "                return node.surface_form, 0.85, \"location phrase detected\"\n",
    "        return \"Unknown\", 0.0, \"location not found\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 15. DISASTER DISTRESS SEVERITY SCORER\n",
    "# ============================================================\n",
    "\n",
    "class DistressSeverityAnalyzer:\n",
    "    \"\"\"\n",
    "    Computes disaster severity using\n",
    "    distress signal density.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: SemanticEvidenceGraph):\n",
    "        self.graph = graph\n",
    "\n",
    "    def severity_score(self):\n",
    "        distress_count = sum(\n",
    "            1 for n in self.graph.nodes if n.signal_type == \"DISTRESS\"\n",
    "        )\n",
    "        return min(0.4 + 0.15 * distress_count, 1.0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 16. EXPLAINABILITY TRACE GENERATOR (VERY IMPORTANT)\n",
    "# ============================================================\n",
    "\n",
    "class ExplainabilityEngine:\n",
    "    \"\"\"\n",
    "    Generates human-readable reasoning traces.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: SemanticEvidenceGraph):\n",
    "        self.graph = graph\n",
    "\n",
    "    def evidence_by_role(self, role):\n",
    "        return [\n",
    "            n.surface_form for n in self.graph.nodes\n",
    "            if n.signal_type == role\n",
    "        ]\n",
    "\n",
    "    def full_trace(self):\n",
    "        trace = {}\n",
    "        for role in SEMANTIC_ROLES:\n",
    "            trace[role] = self.evidence_by_role(role)\n",
    "        return trace\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 17. FULL SEMANTIC DECISION ENGINE (INTEGRATION)\n",
    "# ============================================================\n",
    "\n",
    "class SemanticDecisionEngine:\n",
    "    \"\"\"\n",
    "    End-to-end reasoning from evidence graph to final output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: SemanticEvidenceGraph):\n",
    "        self.graph = graph\n",
    "\n",
    "        self.intent_resolver = PensionIntentResolver(graph)\n",
    "        self.disaster_resolver = DisasterResolver(graph)\n",
    "        self.attribute_resolver = AttributeResolver(graph)\n",
    "        self.distress_analyzer = DistressSeverityAnalyzer(graph)\n",
    "        self.explainer = ExplainabilityEngine(graph)\n",
    "\n",
    "    def decide(self) -> dict:\n",
    "        age, age_c, age_r = self.attribute_resolver.resolve_age()\n",
    "        location, loc_c, loc_r = self.attribute_resolver.resolve_location()\n",
    "\n",
    "        pension, pen_c, pen_r = self.intent_resolver.resolve()\n",
    "        disaster, dis_c, dis_r = self.disaster_resolver.resolve()\n",
    "        distress = self.distress_analyzer.severity_score()\n",
    "\n",
    "        return {\n",
    "            \"Age\": {\"value\": age, \"confidence\": age_c, \"reason\": age_r},\n",
    "            \"Location\": {\"value\": location, \"confidence\": loc_c, \"reason\": loc_r},\n",
    "            \"Pension_Type\": {\n",
    "                \"value\": pension,\n",
    "                \"confidence\": pen_c,\n",
    "                \"reason\": pen_r\n",
    "            },\n",
    "            \"Disaster_Event\": {\n",
    "                \"value\": disaster,\n",
    "                \"confidence\": dis_c,\n",
    "                \"reason\": dis_r\n",
    "            },\n",
    "            \"Disaster_Severity\": distress,\n",
    "            \"Explainability_Trace\": self.explainer.full_trace()\n",
    "        }\n",
    "# ============================================================\n",
    "# PART 4 — SEER-NLP MASTER MODEL & DEPLOYMENT LAYER\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "from typing import Iterable\n",
    "\n",
    "# ============================================================\n",
    "# 18. SEER-NLP MASTER MODEL (FULL PIPELINE)\n",
    "# ============================================================\n",
    "\n",
    "class SEERNLPModel:\n",
    "    \"\"\"\n",
    "    SEER-NLP Master Model\n",
    "\n",
    "    Orchestrates:\n",
    "    - Text normalization\n",
    "    - Adaptive semantic signal extraction\n",
    "    - Evidence graph construction\n",
    "    - Semantic reasoning & conflict resolution\n",
    "    - Explainability generation\n",
    "\n",
    "    This is the object serialized for deployment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Core NLP components\n",
    "        self.normalizer = TextNormalizer()\n",
    "        self.signal_extractor = AdaptiveSemanticSignalExtractor()\n",
    "\n",
    "        # Meta information\n",
    "        self.version = \"SEER-NLP v1.0\"\n",
    "        self.description = (\n",
    "            \"Semantic Explainable Extraction & Reasoning \"\n",
    "            \"for Pension Applications in Disaster Response\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # SINGLE DOCUMENT INFERENCE\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def infer(self, raw_text: str, adapt: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Perform full semantic extraction & reasoning\n",
    "        on a single application narrative.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_text : str\n",
    "            Input pension application text\n",
    "        adapt : bool\n",
    "            Whether to allow lexicon adaptation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict : structured, explainable output\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Normalize\n",
    "        text = self.normalizer.normalize(raw_text)\n",
    "\n",
    "        # Step 2: Optional online adaptation (novelty)\n",
    "        if adapt:\n",
    "            self.signal_extractor.adapt(text)\n",
    "\n",
    "        # Step 3: Extract semantic signals & graph\n",
    "        graph = self.signal_extractor.extract_signals(text)\n",
    "\n",
    "        # Step 4: Reasoning & decision\n",
    "        decision_engine = SemanticDecisionEngine(graph)\n",
    "        result = decision_engine.decide()\n",
    "\n",
    "        # Step 5: Attach meta info\n",
    "        result[\"_meta\"] = {\n",
    "            \"model_version\": self.version,\n",
    "            \"adaptation_used\": adapt\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # BATCH INFERENCE (DATASET SCALE)\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def infer_batch(\n",
    "        self,\n",
    "        texts: Iterable[str],\n",
    "        adapt: bool = False\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Apply SEER-NLP on multiple texts.\n",
    "\n",
    "        This method is optimized for datasets\n",
    "        (e.g., your 10k synthetic dataset).\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for text in texts:\n",
    "            outputs.append(self.infer(text, adapt=adapt))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 19. MODEL SERIALIZATION & LOADING\n",
    "# ============================================================\n",
    "\n",
    "class ModelPersistence:\n",
    "    \"\"\"\n",
    "    Handles saving and loading of SEER-NLP models.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def save(model: SEERNLPModel, path: str):\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"✅ SEER-NLP model saved to: {path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str) -> SEERNLPModel:\n",
    "        with open(path, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"✅ SEER-NLP model loaded from: {path}\")\n",
    "        return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 20. STREAMLIT-READY INTERFACE (API STYLE)\n",
    "# ============================================================\n",
    "\n",
    "class SEERNLPService:\n",
    "    \"\"\"\n",
    "    Lightweight wrapper for app-level usage.\n",
    "    Ideal for Streamlit or REST APIs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str):\n",
    "        self.model = ModelPersistence.load(model_path)\n",
    "\n",
    "    def process_text(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Entry point for Streamlit UI.\n",
    "        \"\"\"\n",
    "        return self.model.infer(text, adapt=False)\n",
    "\n",
    "    def process_text_with_adaptation(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Advanced mode: adapts lexicon online.\n",
    "        \"\"\"\n",
    "        return self.model.infer(text, adapt=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 21. ONE-TIME MODEL BUILD & SAVE SCRIPT\n",
    "# ============================================================\n",
    "\n",
    "def build_and_save_seer_model(\n",
    "    output_path: str = \"seer_nlp_model.pkl\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Initializes SEER-NLP and serializes it.\n",
    "    Run this ONCE before deployment.\n",
    "    \"\"\"\n",
    "    model = SEERNLPModel()\n",
    "    ModelPersistence.save(model, output_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 22. EXAMPLE USAGE (OPTIONAL TEST)\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    I am 72 years old and live in Gaya district.\n",
    "    After floods damaged my house, I lost my husband\n",
    "    and now have no income to survive.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build & save model\n",
    "    build_and_save_seer_model(\"seer_nlp_model.pkl\")\n",
    "\n",
    "    # Load & test\n",
    "    service = SEERNLPService(\"seer_nlp_model.pkl\")\n",
    "    output = service.process_text(sample_text)\n",
    "\n",
    "    import json\n",
    "    print(json.dumps(output, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c11878c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SEER-NLP model saved to: seer_nlp_model.pkl\n",
      "✅ SEER-NLP model loaded from: seer_nlp_model.pkl\n",
      "{\n",
      "    \"Age\": {\n",
      "        \"value\": 72,\n",
      "        \"confidence\": 0.95,\n",
      "        \"reason\": \"explicit age detected\"\n",
      "    },\n",
      "    \"Location\": {\n",
      "        \"value\": \"Unknown\",\n",
      "        \"confidence\": 0.0,\n",
      "        \"reason\": \"location not found\"\n",
      "    },\n",
      "    \"Pension_Type\": {\n",
      "        \"value\": \"Widow Pension\",\n",
      "        \"confidence\": 0.95,\n",
      "        \"reason\": \"resolved by semantic dominance: Widow Pension (2.39) > others\"\n",
      "    },\n",
      "    \"Disaster_Event\": {\n",
      "        \"value\": \"flood\",\n",
      "        \"confidence\": 0.95,\n",
      "        \"reason\": \"disaster energy dominance\"\n",
      "    },\n",
      "    \"Disaster_Severity\": 0.55,\n",
      "    \"Explainability_Trace\": {\n",
      "        \"AGE\": [\n",
      "            \"72 years old\",\n",
      "            \"i am 72 years\"\n",
      "        ],\n",
      "        \"LOCATION\": [],\n",
      "        \"DISASTER\": [\n",
      "            \"flood\",\n",
      "            \"floods\"\n",
      "        ],\n",
      "        \"WIDOW\": [\n",
      "            \"lost my husband\"\n",
      "        ],\n",
      "        \"DISABILITY\": [],\n",
      "        \"OLD_AGE\": [],\n",
      "        \"DISTRESS\": [\n",
      "            \"no income\"\n",
      "        ]\n",
      "    },\n",
      "    \"_meta\": {\n",
      "        \"model_version\": \"SEER-NLP v1.0\",\n",
      "        \"adaptation_used\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PART 4 — SEER-NLP MASTER MODEL & DEPLOYMENT LAYER\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "from typing import Iterable\n",
    "\n",
    "# ============================================================\n",
    "# 18. SEER-NLP MASTER MODEL (FULL PIPELINE)\n",
    "# ============================================================\n",
    "\n",
    "class SEERNLPModel:\n",
    "    \"\"\"\n",
    "    SEER-NLP Master Model\n",
    "\n",
    "    Orchestrates:\n",
    "    - Text normalization\n",
    "    - Adaptive semantic signal extraction\n",
    "    - Evidence graph construction\n",
    "    - Semantic reasoning & conflict resolution\n",
    "    - Explainability generation\n",
    "\n",
    "    This is the object serialized for deployment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Core NLP components\n",
    "        self.normalizer = TextNormalizer()\n",
    "        self.signal_extractor = AdaptiveSemanticSignalExtractor()\n",
    "\n",
    "        # Meta information\n",
    "        self.version = \"SEER-NLP v1.0\"\n",
    "        self.description = (\n",
    "            \"Semantic Explainable Extraction & Reasoning \"\n",
    "            \"for Pension Applications in Disaster Response\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # SINGLE DOCUMENT INFERENCE\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def infer(self, raw_text: str, adapt: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Perform full semantic extraction & reasoning\n",
    "        on a single application narrative.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_text : str\n",
    "            Input pension application text\n",
    "        adapt : bool\n",
    "            Whether to allow lexicon adaptation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict : structured, explainable output\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Normalize\n",
    "        text = self.normalizer.normalize(raw_text)\n",
    "\n",
    "        # Step 2: Optional online adaptation (novelty)\n",
    "        if adapt:\n",
    "            self.signal_extractor.adapt(text)\n",
    "\n",
    "        # Step 3: Extract semantic signals & graph\n",
    "        graph = self.signal_extractor.extract_signals(text)\n",
    "\n",
    "        # Step 4: Reasoning & decision\n",
    "        decision_engine = SemanticDecisionEngine(graph)\n",
    "        result = decision_engine.decide()\n",
    "\n",
    "        # Step 5: Attach meta info\n",
    "        result[\"_meta\"] = {\n",
    "            \"model_version\": self.version,\n",
    "            \"adaptation_used\": adapt\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # BATCH INFERENCE (DATASET SCALE)\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def infer_batch(\n",
    "        self,\n",
    "        texts: Iterable[str],\n",
    "        adapt: bool = False\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Apply SEER-NLP on multiple texts.\n",
    "\n",
    "        This method is optimized for datasets\n",
    "        (e.g., your 10k synthetic dataset).\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for text in texts:\n",
    "            outputs.append(self.infer(text, adapt=adapt))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 19. MODEL SERIALIZATION & LOADING\n",
    "# ============================================================\n",
    "\n",
    "class ModelPersistence:\n",
    "    \"\"\"\n",
    "    Handles saving and loading of SEER-NLP models.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def save(model: SEERNLPModel, path: str):\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"✅ SEER-NLP model saved to: {path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str) -> SEERNLPModel:\n",
    "        with open(path, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"✅ SEER-NLP model loaded from: {path}\")\n",
    "        return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 20. STREAMLIT-READY INTERFACE (API STYLE)\n",
    "# ============================================================\n",
    "\n",
    "class SEERNLPService:\n",
    "    \"\"\"\n",
    "    Lightweight wrapper for app-level usage.\n",
    "    Ideal for Streamlit or REST APIs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str):\n",
    "        self.model = ModelPersistence.load(model_path)\n",
    "\n",
    "    def process_text(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Entry point for Streamlit UI.\n",
    "        \"\"\"\n",
    "        return self.model.infer(text, adapt=False)\n",
    "\n",
    "    def process_text_with_adaptation(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Advanced mode: adapts lexicon online.\n",
    "        \"\"\"\n",
    "        return self.model.infer(text, adapt=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 21. ONE-TIME MODEL BUILD & SAVE SCRIPT\n",
    "# ============================================================\n",
    "\n",
    "def build_and_save_seer_model(\n",
    "    output_path: str = \"seer_nlp_model.pkl\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Initializes SEER-NLP and serializes it.\n",
    "    Run this ONCE before deployment.\n",
    "    \"\"\"\n",
    "    model = SEERNLPModel()\n",
    "    ModelPersistence.save(model, output_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 22. EXAMPLE USAGE (OPTIONAL TEST)\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    I am 72 years old and live in Gaya district.\n",
    "    After floods damaged my house, I lost my husband\n",
    "    and now have no income to survive.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build & save model\n",
    "    build_and_save_seer_model(\"seer_nlp_model.pkl\")\n",
    "\n",
    "    # Load & test\n",
    "    service = SEERNLPService(\"seer_nlp_model.pkl\")\n",
    "    output = service.process_text(sample_text)\n",
    "\n",
    "    import json\n",
    "    print(json.dumps(output, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50f72efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AGE': ['72 years old', 'i am 72 years'],\n",
       " 'LOCATION': [],\n",
       " 'DISASTER': ['flood', 'floods'],\n",
       " 'WIDOW': ['husband passed away'],\n",
       " 'DISABILITY': [],\n",
       " 'OLD_AGE': [],\n",
       " 'DISTRESS': ['no income']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"Explainability_Trace\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
